{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Solutions with LLMs and Retrival Augmented Geenration (RAG): Introduction\n",
    "\n",
    "In this workshop we will use notebooks and Python scripts to interactively learn about Large Language Models and RAG.\n",
    "\n",
    "Large Language Models (LLMs) are a type of machine learning models designed to understand and generate human language. They are trained on massive datasets of text to predict and generate language based on given prompts, learning patterns, structures, and relationships in text to produce human-like responses. They can be used to generate text, answer questions, and more.\n",
    "\n",
    "\n",
    "### Getting started with Jupyter Notebooks\n",
    "Notebooks are one of the most imporant tools in teh toolbelt of a data scientist of ML engineer.\n",
    "They are great for interacting with code and visualizing results.\n",
    "This file you are interacting with is a jupyter notebook. In Jupter you can have cells of either text or code.\n",
    "Cells with code can be run by pressing ctrl+enter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Interact with the cell below and run it multiple times to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a if it does not exist\n",
    "if  'a' not in globals():\n",
    "    a = 42\n",
    "\n",
    "a = a+1\n",
    "\n",
    "# press ctrl+enter to run the cell, do that multiple times\n",
    "# notice how the variable `a` is persistent in the cell\n",
    "# you can run full programs in a notebook.\n",
    "\n",
    "# Task 1.2 \n",
    "# To restart the kernel of the notebook you can use the restart button in the top right corner of the screen\n",
    "# Do that and run the cell again to see that the value of a goes back to 43\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralAI\n",
    "\n",
    "In this workshop, we will be using Mistral langauge models, which are similar in concept to OpenAI's ChatGPT and Anthropic's Claude.\n",
    "\n",
    "To start working with Mistral, you first need to install the library. You can install any library in a notebook like the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from mistralai) (0.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from mistralai) (0.27.2)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from mistralai) (1.0.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from mistralai) (2.9.2)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from mistralai) (2.8.2)\n",
      "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from mistralai) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from python-dateutil==2.8.2->mistralai) (1.16.0)\n",
      "Requirement already satisfied: anyio in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (4.12.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/jean.machado@getyourguide.com/prj/rag-workshop/.venv/lib/python3.12/site-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting access\n",
    "LLMs are usually big and use a lot of computer resourcesthe most common way to use them is to run them in teh cloud via network calls.\n",
    "For the sake of this workshop we will use the cloud version as we dont need to download big models.\n",
    "If you are interested in running llms locally check [ollama](https://github.com/ollama/ollama), arguably the most advanced project that does it.\n",
    "\n",
    "\n",
    "#### API key\n",
    "The second  step is to get a Mistral api key. You can find some APIs keys we prepared for this workshop in this [sheet](https://docs.google.com/spreadsheets/d/1ZwTpkG6OOuVrOx8nzPmgai_7Hwpo8Kun7yZrOmg_5K4/edit?gid=0#gid=0). Get the key (please write your name next to it in the sheet such that people know it is taken) and write it to the .env file using the command\n",
    "\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Use the cell below to write your api key into the file so you can use mistral.\n",
    "We write the variable into a file named .env. .env files a are a standard in python to load information like api keys and passwords that should not stay with the code for security reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file .env\n",
    "\n",
    "mistral_api_key = 'KEY PLACEHOLDER - REPLACE WITH YOUR KEY'\n",
    "with open('../.env', 'w') as f:\n",
    "    f.write(f'MISTRAL_API_KEY={mistral_api_key}')\n",
    "\n",
    "# make sure to delete teh API key from this cell before commiting the file so you dont save your key in the repo which is a security risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing if it worked\n",
    "Now run the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now reload the the configuration file and it should show your key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# Initialize models and RAG\n",
    "env_file = find_dotenv()\n",
    "print(f\"Loading environment variables from {env_file}\")\n",
    "load_dotenv(env_file)\n",
    "\n",
    "import os\n",
    "env =os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# you should see your key here not\n",
    "print(\"The key is: \", env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mistral\n",
    "Let's run the code below to import Mistral and initialize the Mistral client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Retrieve the Mistral API key from the environment variables\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "# The model below is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\"\n",
    "\n",
    "# The code below defines a function `call_mistral_model` that sends a message to a Mistral model and returns the model's response text.\n",
    "def call_mistral_model(message):\n",
    "    response = mistral_client.chat.complete(\n",
    "        model = model_name,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message,\n",
    "            }\n",
    "            ]\n",
    "        )\n",
    "    # Extract only the text from the response\n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out\n",
    "response = call_mistral_model(\"hello! What is your name?\")\n",
    "# Print the response from the Mistral model\n",
    "print(response)\n",
    "\n",
    "## not how the result resembles natural language. Its the exact same concept as chatgpt.\n",
    "## feel free to play around with the prompt and see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate Limiting\n",
    "\n",
    "When using APIs, you might encounter rate limiting. Rate limiting is a mechanism implemented by APIs to control the number of requests a user can make in a given time frame. This is done to prevent abuse so 1 player dont make the whole system worse.\n",
    "However, rate limiting can be annoying because it can interrupt your workflow and force you to wait before making more requests.\n",
    "\n",
    "To make things easier, we've implemented a LargeLanguageModel class (see usage example below) that automatically adds sleep intervals between requests to avoid exceeding the rate limit. We will use the LargeLanguageModel class moving forward to make calls to Mistral AI. This class has the same logic as the examples we showed above but includes a mechanism to counter the rate limiting issue. \n",
    "\n",
    "You don't need to understand all the code in the class to use it. Feel free to have a look teh class by ctrl+ clicking in LargeLanguageModel if you are curious.\n",
    "There are libraries that deal with rate limiting out of the box like [lang-chain](https://github.com/hwchase17/langchain) (more on it later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.llm import LargeLanguageModel\n",
    "\n",
    "# Initialize an instance of the LargeLanguageModel class\n",
    "llm = LargeLanguageModel()\n",
    "\n",
    "# Make a call to Mistral using the LargeLanguageModel class\n",
    "# This class includes logic to counteract rate limiting by adding appropriate sleep intervals between requests\n",
    "response = llm.call(\"hello! What is your name?\")\n",
    "\n",
    "# Print the response from the Mistral model\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the LLM\n",
    "Now that we have seen how to make a basic call to the Mistral model using the `LargeLanguageModel` class, let's try some more prompts to see how the model responds to different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Exploring and Modifying Prompts\n",
    "Below are some example use cases of how to use an LLM such as Mistral. Play around with the prompts and see the results. Modify the prompts to see how the model's responses change. This will help you understand how to craft effective prompts and get the desired output from the model.\n",
    "\n",
    "Try to:\n",
    "- Ask different types of questions\n",
    "- Change the text for summarization or extraction (see examples 2 and 3 below)\n",
    "- Alter the style of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Asking for Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! 42Berlin is a coding school that follows the educational model pioneered by Ã‰cole 42 in France. Here are some key features and aspects of 42Berlin:\n",
      "\n",
      "### Educational Model\n",
      "- **Peer-to-Peer Learning**: The school emphasizes collaborative learning, where students help and teach each other. This fosters a community-driven environment and encourages a deep understanding of concepts.\n",
      "- **Project-Based**: The curriculum is heavily project-based, with students working on various coding and programming projects to apply what they've learned.\n",
      "- **No Traditional Classes or Teachers**: There are no formal lectures or traditional teachers. Instead, students learn through hands-on projects and interactive problem-solving.\n",
      "\n",
      "### Admission Process\n",
      "- **Piscine (Pool)**: The admission process involves a rigorous, intensive selection phase called the \"Piscine\" (Pool), which typically lasts several weeks. During this time, applicants work on coding challenges and projects to demonstrate their aptitude and passion for programming.\n",
      "- **Open to All**: 42Berlin is open to applicants of all backgrounds and ages. Prior experience in programming is not required, but a strong motivation and aptitude for learning are crucial.\n",
      "\n",
      "### Curriculum\n",
      "- **Core Programming Concepts**: The curriculum covers core programming concepts, algorithms, data structures, and various programming languages.\n",
      "- **Specializations**: As students progress, they can specialize in areas such as web development, mobile app development, data science, and more.\n",
      "- **Real-World Projects**: Students work on real-world projects, often in collaboration with industry partners, to gain practical experience.\n",
      "\n",
      "### Community and Support\n",
      "- **24/7 Access**: The school offers 24/7 access to its facilities, allowing students to work at their own pace and on their own schedule.\n",
      "- **Mentoring**: While there are no traditional teachers, experienced developers and mentors are available to guide and support students.\n",
      "- **Global Network**: As part of the broader 42 Network, students have access to a global community of peers, alumni, and industry professionals.\n",
      "\n",
      "### Outcomes\n",
      "- **Job Placement**: 42Berlin has strong connections with the tech industry, and many graduates secure jobs as software developers, engineers, and other tech roles.\n",
      "- **Career Development**: The school provides support for career development, including resume workshops, interview practice, and networking opportunities.\n",
      "\n",
      "### Location\n",
      "- **Berlin, Germany**: 42Berlin is situated in Berlin, a vibrant tech hub with a thriving startup scene and a strong community of developers and innovators.\n",
      "\n",
      "### Costs\n",
      "- **Tuition-Free**: 42Berlin follows the tuition-free model of Ã‰cole 42, making it an accessible option for those who may not be able to afford traditional educational programs.\n",
      "\n",
      "Overall, 42Berlin offers a unique and innovative approach to learning programming and software development, focusing on practical skills, collaboration, and real-world experience.\n"
     ]
    }
   ],
   "source": [
    "# Example: Asking for information\n",
    "prompt = \"Can you tell me about coding school 42Berlin?\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Summarizing a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42Berlin is a tuition-free, non-profit coding school empowering the next generation of software engineers through inclusive, peer-learning education, aiming for Master's level competency.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_summarize = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central NeukÃ¶lln, we train our students up to the equivalent of Masterâ€™s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Summarize the following text in one brief sentence: {text_to_summarize}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Extracting Information from a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year 42Berlin was founded is 2021.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_extract_from = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central NeukÃ¶lln, we train our students up to the equivalent of Masterâ€™s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Extract the year 42Berlin was founded from the following text: {text_to_extract_from}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 3.1: Doing maths -> When will the LLM make a mistake?\n",
    "\n",
    "LLMs are trained on text language so they are not necessarily good with maths. We can be confident that it will fail to produce the correct result if you ask a question that is complex enough. On the notebook below we increase the complexity of the task at every run. Run it until you see it diverging.\n",
    "\n",
    "What was te number of iterations necessary to make it break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct result:  827240261886336764177\n",
      "Mistral result:  The result of 17 raised to the power of 17 (17^17) is a very large number. Here it is:\n",
      "\n",
      "17^17 = 4,299,816,961,406,251,776,668,926,984,275,280,160,745,629,199,685,440,162,560,608,000,000\n",
      "\n",
      "This number has 53 digits.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'a' not in globals():\n",
    "    a = 1\n",
    "    b = 1\n",
    "a = a + 1\n",
    "b = b + 1\n",
    "\n",
    "response = llm.call(f\"what is the results of {a} ** {b}\")\n",
    "\n",
    "print(\"Correct result: \", a**b)\n",
    "print(\"Mistral result: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination\n",
    "LLMs sometimes generate responses that are plausible-sounding but factually incorrect or nonsensical. This phenomenon is known as \"hallucination\". \n",
    "Hallucination can occur because the model generates text based on patterns in the training data rather than actual knowledge or retrieval of relevant information.\n",
    "LLMs will produce the most likelly words that they would find in similar text, they have no clue about fact vs fiction. They can confidently produce writing about things they have no clue about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Demonstrating Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will ask the model a question that it might to hallucinate an answer for, showing the limitations of relying solely on language generation without retrieval.\n",
    "\n",
    "Try running the command below a few times in a row and see how the response by the LLM changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response likely to hallucinate:\n",
      "\n",
      "The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps Workshop for Women.\" This event was designed to empower women in the field of Machine Learning and Operations (MLOps) by providing practical skills, networking opportunities, and mentorship. It is part of both organizations' efforts to increase diversity and inclusion in the tech industry.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question likely to cause hallucination\n",
    "prompt = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "response = llm.call(prompt)\n",
    "print(\"Response likely to hallucinate:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to the next section on Retrieval-Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Large language models (LLMs) can sometimes hallucinate, presenting false information due to outdated training data. Retrieval-Augmented Generation (RAG) allows us to incorporate external information to mitigate these challenges. \n",
    "\n",
    "In RAG, a retrieval component searches and retrieves relevant information from a knowledge base or external documents, and a generation component uses this information to generate responses.\n",
    "This approach allows the model to access up-to-date information and provide more detailed and accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will demonstrate a simple example of how to use Retrieval-Augmented Generation. We will use a predefined set of documents, retrieve relevant information based on a query, and then generate a response using the retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message: str, context: str):\n",
    "    \"\"\"\n",
    "    Message is the question that the user is asking.\n",
    "    Context is the information that we want to use to answer the question.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code in the cell below, you can compare how our LLM responses differ by the information that you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"Machine Learning Operations (MLOps) Workshop.\" This event aimed to provide participants with practical insights and hands-on experiences in the field of MLOps, highlighting the importance of operationalizing machine learning models in a scalable and efficient manner.\n",
      "Error happened while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Waiting 2 seconds before retrying\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " I don't know\n"
     ]
    }
   ],
   "source": [
    "# The workshop the MLOps Community hosted together with Girls in Tech Germany was called \"AI Launchpad: Building Your First Ml Pipeline\" or simply \"Building Your First ML Pipeline\"\n",
    "# We copy paste the info from our Eventbrite event page from the previous workshop and use this as context for the model to retrieve the right info from\n",
    "context = \"\"\"\n",
    "AI Launchpad - Building Your First ML Pipeline: \n",
    "On Wednesday, June 5th, 2024, the MLOps Community Berlin in collaboration with Girls in Tech Germany hosted an interactive workshop for beginners who want to kick start their career in AI/ML. \n",
    "The workshop starts at 18.00h at 42Berlin. \n",
    "\n",
    "ðŸ” Why Attend?\n",
    "\n",
    "Gain hands-on experience building your first ML pipeline in an agile way\n",
    "Apply the fundamentals of statistical modeling and basic Python\n",
    "Opportunities to improve your portfolio \n",
    "Connect with ML professionals at different levels of seniority\n",
    "\n",
    "\n",
    "âœ¨ The Agenda: \n",
    "\n",
    "6:00 pm - Arrive & Pizza \n",
    "6:30 pm -  Introduction MLOps and GiT\n",
    "6:45 pm - Workshop Introduction\n",
    "7:30 pm - Break\n",
    "7:45 pm - Workshop\n",
    "9:45 pm - Networking\n",
    "\n",
    "\n",
    "ðŸŽ‰ Highlights:\n",
    "\n",
    "Food and drinks provided\n",
    "Engaging discussions and networking opportunities\n",
    "Bring your laptop and get ready to learn!\n",
    "\n",
    "\n",
    "ðŸ’¼ Who Should Attend?\n",
    "\n",
    "Individuals starting their career in Machine Learning or Artificial Intelligence\n",
    "Those looking to transition into the field of AI/ML\n",
    "Anyone interested in contributing to and learning from the ML community\n",
    "Don't miss out on this chance to gain practical AI/ML skills while expanding your professional network! \n",
    "\"\"\"\n",
    "\n",
    "message = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error happened while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Waiting 2 seconds before retrying\n",
      "GENERIC RESPONSE:\n",
      " I'm an assistant that operates solely on the data it has been trained on up until 2021, and I don't have real-time web browsing capabilities or the ability to predict future weather with certainty. Therefore, I can't provide the specific weather for Berlin on the 10th of December, 2027.\n",
      "\n",
      "For the most accurate information, I recommend checking a reliable weather forecasting website or service closer to the date.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " Hello! On the 10th of December 2027, the weather in Berlin is expected to be around 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same thing with Berlin weather data!\n",
    "context = \"\"\"\n",
    "The weather in Berlin  December of 2027 will be around 13 degrees Celsius.\n",
    "Specific dates:\n",
    "- 10th of December: 10 degrees Celsius\n",
    "- 15th of December: 15 degrees Celsius\n",
    "- 20th of December: 7 degrees Celsius\n",
    "\"\"\"\n",
    "\n",
    "message = \"What will be the weather in Berlin on the 10th of December of 2027?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it yourself! Can you find some content on the internet (think, for example, news articles or very specific, locally relevant information that the LLM normally would not have access to). \n",
    "\n",
    "Play around with it and let the creative juices flow. Can you discover some more use cases for which you can use RAG can help make our LLM smarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " Q: What is the difference between a software engineer and a software developer?\n",
      "\n",
      "A: The terms \"software engineer\" and \"software developer\" are often used interchangeably, but there can be some differences in the context of their roles and responsibilities.\n",
      "\n",
      "1. **Education and Background**:\n",
      "   - **Software Engineer**: Often has a formal education in software engineering or a related field. They may have a Bachelor's or Master's degree in software engineering, computer science, or a similar discipline.\n",
      "   - **Software Developer**: May have a degree in computer science or a related field, but the educational background can vary. Some developers may be self-taught or have degrees in unrelated fields but have acquired relevant skills through experience.\n",
      "\n",
      "2. **Approach and Methodologies**:\n",
      "   - **Software Engineer**: Tends to focus on the engineering aspects of software development, including system design, algorithms, data structures, and software architecture. They often follow structured methodologies like Agile, Scrum, or Waterfall.\n",
      "   - **Software Developer**: Primarily focuses on writing code and may have a broader range of responsibilities, including issues related to the development environment, tools, and frameworks. They might work on a variety of tasks, including coding, debugging, and maintenance.\n",
      "\n",
      "3. **Scope of Work**:\n",
      "   - **Software Engineer**: May work on larger, more complex systems and be involved in architectural decisions, performance optimization, and ensuring the software is maintainable and scalable.\n",
      "   - **Software Developer**: Typically works on specific features or components of a software project. They may work on both front-end and back-end development or specialize in one area.\n",
      "\n",
      "4. **Certifications and Standards**:\n",
      "   - **Software Engineer**: Might pursue certifications related to software engineering principles and best practices, such as those offered by the IEEE or other professional organizations.\n",
      "   - **Software Developer**: May focus on certifications related to specific technologies or programming languages, like Microsoft Certified: Azure Developer Associate or certifications from Oracle, AWS, etc.\n",
      "\n",
      "5. **Career Progression**:\n",
      "   - **Software Engineer**: May aim for roles such as Software Architect, Technical Lead, or Engineering Manager, where they drive the architectural and design aspects of software projects.\n",
      "   - **Software Developer**: May advance to roles like Senior Developer, Team Lead, or specialize in a particular technology stack, such as becoming a Full-Stack Developer or a DevOps Engineer.\n",
      "\n",
      "In many organizations, the titles \"Software Engineer\" and \"Software Developer\" might be used interchangeably, and the specific roles and responsibilities can vary. However, understanding the nuances can help in positioning oneself for specific career paths and responsibilities.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " Sure, I'd be happy to help!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"\"\"\"\"\" # Add your context here\n",
    "\n",
    "message = \"\" # Add your message here\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! \n",
    "\n",
    "RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, such as PDFs, Google search results, social media posts, and more. With that, weâ€™ve built a simple Q&A RAG.\n",
    "\n",
    "## ===> Now head to notebook 2 on embedddings and how llms undertand language\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
