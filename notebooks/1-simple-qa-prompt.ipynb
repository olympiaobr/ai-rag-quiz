{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Solutions with LLMs and RAG: Introduction\n",
    "\n",
    "In this workshop we will use notebooks and Python scripts to interactively learn about Large Language Models and RAG.\n",
    "\n",
    "Large Language Models (LLMs) are a type of machine learning models designed to understand and generate human language. They are trained on massive datasets of text to predict and generate language based on given prompts, learning patterns, structures, and relationships in text to produce human-like responses. They can be used to generate text, answer questions, and more.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines language generation with real-time data retrieval, allowing models to access external sources or databases to provide more accurate, contextually relevant answers.\n",
    "\n",
    "RAG combines two main components:\n",
    "- Retrieval: This component searches and retrieves relevant information from external databases or documents.\n",
    "- Generation: This component uses the retrieved information to generate more accurate and contextually relevant responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Jupyter notebook\n",
    "First of all, let's make sure you understand the Jupyter notebook interface.\n",
    "In Jupter you can have cells of either text or code.\n",
    "You can type any python code in a cell and press shift + enter to run it.\n",
    "\n",
    "Interact with the cell below and run it multiple times to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "a = 42 \n",
    "a = a + 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also import libraries and use them in the cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-11-20 17:34:06'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MistralAI\n",
    "\n",
    "In this workshop, we will be using MistralAI's LLMs, which are similar in concept to OpenAI's ChatGPT and Anthropic's Claude.\n",
    "\n",
    "To start working with Mistral, you first need to install the library. We did that for you already.\n",
    "\n",
    "```bash\n",
    "pip install mistralai\n",
    "```\n",
    "\n",
    "\n",
    "The second  step is to get a Mistral api key. You can find some APIs keys we prepared for this workshop in this [sheet](https://docs.google.com/spreadsheets/d/1ZwTpkG6OOuVrOx8nzPmgai_7Hwpo8Kun7yZrOmg_5K4/edit?gid=0#gid=0). Get the key (please write your name next to it in the sheet such that people know it is taken) and write it to the .env file using the command\n",
    "\n",
    "```\n",
    "echo 'MISTRAL_API_KEY=\"your_api_key_here\"' >> .env\n",
    "```\n",
    "\n",
    "You can run LLMs on your local machine or use the cloud version.\n",
    "For the sake of this workshop we will use the cloud version as we dont need to download big models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the code below to import Mistral and initialize the Mistral client: <br> _(Note: If you run into a ModuleNotFoundError when trying to run the code below, run the command pip install -r requirements.txt in your terminal and try again after it finishes)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Retrieve the Mistral API key from the environment variables\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "# The model below is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function `call_mistral_model` that sends a message to a Mistral model and returns the model's response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral_model(message):\n",
    "    response = mistral_client.chat.complete(\n",
    "        model = model_name,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message,\n",
    "            }\n",
    "            ]\n",
    "        )\n",
    "    # Extract only the text from the response\n",
    "    response_text = response.choices[0].message.content\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out\n",
    "response = call_mistral_model(\"hello! What is your name?\")\n",
    "\n",
    "# Print the response from the Mistral model\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Rate Limiting\n",
    "\n",
    "When using APIs, you might encounter rate limiting. Rate limiting is a mechanism implemented by APIs to control the number of requests a user can make in a given time frame. This is done to prevent abuse and ensure fair use of the API resources. \n",
    "\n",
    "However, rate limiting can be annoying because it can interrupt your workflow and force you to wait before making more requests.\n",
    "\n",
    "To make things easier, we've implemented a LargeLanguageModel class (see usage example below) that automatically adds sleep intervals between requests to avoid exceeding the rate limit. We will use the LargeLanguageModel class moving forward to make calls to Mistral AI. This class has the same logic as the examples we showed above but includes a mechanism to counter the rate limiting issue. \n",
    "\n",
    "You don't need to understand all the code in the class, but feel free to have a look in chat_solution.llm in case you're curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.llm import LargeLanguageModel\n",
    "\n",
    "# Initialize an instance of the LargeLanguageModel class\n",
    "llm = LargeLanguageModel()\n",
    "\n",
    "# Make a call to Mistral using the LargeLanguageModel class\n",
    "# This class includes logic to counteract rate limiting by adding appropriate sleep intervals between requests\n",
    "response = llm.call(\"hello! What is your name?\")\n",
    "\n",
    "# Print the response from the Mistral model\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the LLM\n",
    "Now that we have seen how to make a basic call to the Mistral model using the `LargeLanguageModel` class, let's try some more prompts to see how the model responds to different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploring and Modifying Prompts\n",
    "Below are some example use cases of how to use an LLM such as Mistral. Play around with the prompts and see the results. Modify the prompts to see how the model's responses change. This will help you understand how to craft effective prompts and get the desired output from the model.\n",
    "\n",
    "Try to:\n",
    "- Ask different types of questions\n",
    "- Change the text for summarization or extraction (see examples 2 and 3 below)\n",
    "- Alter the style of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Asking for Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42Berlin is a tuition-free coding school that follows the innovative and intensive educational model pioneered by 42 Network, which was founded in France by Xavier Niel, Nicolas Sadirac, and Florian Bucher in 2013. The 42 Network now includes several campuses around the world, including 42Berlin, which opened in 2017.\n",
      "\n",
      "Here are some key features of 42Berlin:\n",
      "\n",
      "### No Tuition Fees\n",
      "42Berlin does not charge tuition fees. Instead, it relies on sponsorships from tech companies and other organizations to fund its operations. This makes it an accessible option for students who might not be able to afford traditional educational programs.\n",
      "\n",
      "### Peer-to-Peer Learning\n",
      "The school emphasizes peer-to-peer learning. Students are encouraged to help each other and work collaboratively on projects. This approach aims to foster a sense of community and teamwork, which are essential skills in the tech industry.\n",
      "\n",
      "### Project-Based Curriculum\n",
      "The curriculum is heavily project-based. Students learn by doing, working on a series of increasingly complex programming projects. This hands-on approach helps students gain practical experience and develop problem-solving skills.\n",
      "\n",
      "### 24/7 Access\n",
      "The school offers 24/7 access to its facilities. Students can work and learn at their own pace, which is particularly beneficial for those who need flexible schedules.\n",
      "\n",
      "### No Traditional Classes or Teachers\n",
      "42Berlin does not have traditional classes or teachers. Instead, students follow a structured curriculum using online resources and are guided by a team of mentors who provide support and feedback.\n",
      "\n",
      "### Duration\n",
      "The program typically takes around 3 to 5 years to complete, depending on the student's pace. However, the intense and immersive nature of the program often allows students to finish sooner.\n",
      "\n",
      "### Selection Process\n",
      "Admission to 42Berlin is competitive. Applicants go through a selection process that includes an online aptitude test and a series of challenges designed to assess their problem-solving skills and potential for success in the program.\n",
      "\n",
      "### Career Prospects\n",
      "Graduates of 42Berlin often go on to pursue careers in software development, web development, data science, and other tech-related fields. The school has strong relationships with tech companies and startups, which can help graduates find jobs.\n",
      "\n",
      "### Community and Networking\n",
      "Being part of the 42 Network provides students with access to a global community of peers, mentors, and alumni. This can be invaluable for networking, collaboration, and professional development.\n",
      "\n",
      "Overall, 42Berlin offers a unique and challenging educational experience that can be particularly appealing to those who are self-motivated, enjoy coding, and thrive in collaborative environments.\n"
     ]
    }
   ],
   "source": [
    "# Example: Asking for information\n",
    "prompt = \"Can you tell me about coding school 42Berlin?\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Summarizing a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"42Berlin is a tuition-free coding school empowering the next generation of coders through accessible and inclusive education, offering Master's-level training in central Neuk√∂lln.\"\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_summarize = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neuk√∂lln, we train our students up to the equivalent of Master‚Äôs level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Summarize the following text in one brief sentence: {text_to_summarize}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Extracting Information from a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year 42Berlin was founded is 2021.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_extract_from = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neuk√∂lln, we train our students up to the equivalent of Master‚Äôs level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Extract the year 42Berlin was founded from the following text: {text_to_extract_from}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination\n",
    "LLMs sometimes generate responses that are plausible-sounding but factually incorrect or nonsensical. This phenomenon is known as \"hallucination\". \n",
    "\n",
    "Hallucination can occur because the model generates text based on patterns in the training data rather than actual knowledge or retrieval of relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Demonstrating Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will ask the model a question that it might to hallucinate an answer for, showing the limitations of relying solely on language generation without retrieval.\n",
    "\n",
    "Try running the command below a few times in a row and see how the response by the LLM changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response likely to hallucinate:\n",
      "\n",
      "The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Women.\" This initiative aimed to empower women in the field of machine learning operations (MLOps) by providing educational resources, networking opportunities, and hands-on training.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question likely to cause hallucination\n",
    "prompt = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "response = llm.call(prompt)\n",
    "print(\"Response likely to hallucinate:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to the next section on Retrieval-Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Large language models (LLMs) can sometimes hallucinate, presenting false information due to outdated training data. Retrieval-Augmented Generation (RAG) allows us to incorporate external information to mitigate these challenges. \n",
    "\n",
    "In RAG, a retrieval component searches and retrieves relevant information from a knowledge base or external documents, and a generation component uses this information to generate responses.\n",
    "This approach allows the model to access up-to-date information and provide more detailed and accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will demonstrate a simple example of how to use Retrieval-Augmented Generation. We will use a predefined set of documents, retrieve relevant information based on a query, and then generate a response using the retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message: str, context: str):\n",
    "    \"\"\"\n",
    "    Message is the question that the user is asking.\n",
    "    Context is the information that we want to use to answer the question.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code in the cell below, you can compare how our LLM responses differ by the information that you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Beginners\". This event aimed to introduce participants to the principles of MLOps (Machine Learning Operations) and provide practical insights into the field.\n",
      "Error happended while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Waiting 2 seconds before retrying\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " The name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech is \"Building Your First ML Pipeline\".\n"
     ]
    }
   ],
   "source": [
    "# The workshop the MLOps Community hosted together with Girls in Tech Germany was called \"AI Launchpad: Building Your First Ml Pipeline\" or simply \"Building Your First ML Pipeline\"\n",
    "# We copy paste the info from our Eventbrite event page from the previous workshop and use this as context for the model to retrieve the right info from\n",
    "context = \"\"\"\n",
    "AI Launchpad - Building Your First ML Pipeline: \n",
    "On Wednesday, June 5th, 2024, the MLOps Community Berlin in collaboration with Girls in Tech Germany hosted an interactive workshop for beginners who want to kick start their career in AI/ML. \n",
    "The workshop starts at 18.00h at 42Berlin. \n",
    "\n",
    "üîç Why Attend?\n",
    "\n",
    "Gain hands-on experience building your first ML pipeline in an agile way\n",
    "Apply the fundamentals of statistical modeling and basic Python\n",
    "Opportunities to improve your portfolio \n",
    "Connect with ML professionals at different levels of seniority\n",
    "\n",
    "\n",
    "‚ú® The Agenda: \n",
    "\n",
    "6:00 pm - Arrive & Pizza \n",
    "6:30 pm -  Introduction MLOps and GiT\n",
    "6:45 pm - Workshop Introduction\n",
    "7:30 pm - Break\n",
    "7:45 pm - Workshop\n",
    "9:45 pm - Networking\n",
    "\n",
    "\n",
    "üéâ Highlights:\n",
    "\n",
    "Food and drinks provided\n",
    "Engaging discussions and networking opportunities\n",
    "Bring your laptop and get ready to learn!\n",
    "\n",
    "\n",
    "üíº Who Should Attend?\n",
    "\n",
    "Individuals starting their career in Machine Learning or Artificial Intelligence\n",
    "Those looking to transition into the field of AI/ML\n",
    "Anyone interested in contributing to and learning from the ML community\n",
    "Don't miss out on this chance to gain practical AI/ML skills while expanding your professional network! \n",
    "\"\"\"\n",
    "\n",
    "message = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " I cannot accurately predict the weather for a specific date in the future, as weather forecasts are typically made a few days to a week in advance and can change significantly over time. For the most accurate and up-to-date information, I recommend checking a reliable weather forecast service closer to the date you're interested in.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " On the 10th of December 2027, the weather in Berlin will be around 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same thing with Berlin weather data!\n",
    "context = \"\"\"\n",
    "The weather in Berlin  December of 2027 will be around 13 degrees Celsius.\n",
    "Specific dates:\n",
    "- 10th of December: 10 degrees Celsius\n",
    "- 15th of December: 15 degrees Celsius\n",
    "- 20th of December: 7 degrees Celsius\n",
    "\"\"\"\n",
    "\n",
    "message = \"What will be the weather in Berlin on the 10th of December of 2027?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it yourself! Can you find some content on the internet (think, for example, news articles or very specific, locally relevant information that the LLM normally would not have access to). \n",
    "\n",
    "Play around with it and let the creative juices flow. Can you discover some more use cases for which you can use RAG can help make our LLM smarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " Question: What is the difference between a human brain and a computer brain?\n",
      "\n",
      "Computer brains, also known as artificial neural networks (ANNs), are designed to mimic the structure and function of the human brain to some extent. However, there are several key differences between the two:\n",
      "\n",
      "1. **Hardware and Structure**:\n",
      "   - **Human Brain**: The brain is a physical organ consisting of neurons, synapses, and various types of cells. It has a complex, hierarchical structure with different regions specialized for different functions.\n",
      "   - **Computer Brain**: Computer brains are digital systems composed of processors, memory, and software algorithms. They are typically more modular and less organically complex than biological brains.\n",
      "\n",
      "2. **Learning and Adaptation**:\n",
      "   - **Human Brain**: Learning in the human brain involves complex biological processes, including synaptic plasticity and neurogenesis. It can adapt to new information and experiences continuously.\n",
      "   - **Computer Brain**: Learning in a computer brain is based on mathematical models and algorithms. It can adapt through processes like backpropagation in neural networks, but it lacks the biological complexity and flexibility of human learning.\n",
      "\n",
      "3. **Processing Capacity and Efficiency**:\n",
      "   - **Human Brain**: The human brain is highly efficient at processing information, especially in tasks that involve pattern recognition, creativity, and intuition. It consumes about 20 watts of power.\n",
      "   - **Computer Brain**: While computers can process vast amounts of data quickly, they struggle with tasks that require human-like intuition, creativity, and common sense. They also consume more power, especially for high-performance tasks.\n",
      "\n",
      "4. **Consciousness and Subjective Experience**:\n",
      "   - **Human Brain**: Humans possess consciousness, subjective experiences, and a sense of self, which are not yet fully understood scientifically.\n",
      "   - **Computer Brain**: Computers do not have consciousness, subjective experiences, or a sense of self. They operate based on pre-programmed instructions and algorithms.\n",
      "\n",
      "5. **Flexibility and Generalization**:\n",
      "   - **Human Brain**: The human brain is extremely flexible and can generalize from limited data. It can transfer learning from one domain to another.\n",
      "   - **Computer Brain**: Computer brains require large amounts of data for training and may struggle to generalize to new, unseen data. They are often less flexible than human brains.\n",
      "\n",
      "6. **Error Tolerance**:\n",
      "   - **Human Brain**: The human brain is robust and can tolerate errors and noise. It can still function reasonably well even with damaged or missing parts.\n",
      "   - **Computer Brain**: Computers are less tolerant of errors. A single hardware or software error can cause significant issues or even system failure.\n",
      "\n",
      "7. **Ethical and Social Implications**:\n",
      "   - **Human Brain**: The human brain is subject to various ethical and social considerations, such as privacy, consent, and human rights.\n",
      "   - **Computer Brain**: While not subject to the same ethical considerations as human brains, computer brains raise their own set of ethical and social concerns, such as bias, accountability, and job displacement due to automation.\n",
      "\n",
      "In summary, while computer brains can mimic some aspects of the human brain, they differ significantly in their structure, learning processes, processing capabilities, consciousness, flexibility, error tolerance, and ethical implications.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " Sure, what's your question?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"\"\"\"\"\" # Add your context here\n",
    "\n",
    "message = \"\" # Add your message here\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! \n",
    "\n",
    "RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, such as PDFs, Google search results, social media posts, and more. With that, we‚Äôve built a simple Q&A RAG.\n",
    "\n",
    "In the next notebook, we will scale it up to include even more context as well as embeddings to improve the performance of the RAG system further. Go to the notebook \"2-prepare-embedding-data.ipynb\" to dive into the world of embeddings and vector databases!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
