{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Embeddings for RAG Systems\n",
    "\n",
    "In the first notebook, we explored how Large Language Models work and the basics of RAG (Retrieval Augmented Generation). We learned that RAG systems enhance LLM responses by retrieving relevant context from our own data before generating answers.\n",
    "\n",
    "Now, we'll go into the core component that makes RAG possible: embeddings. We'll learn how to convert text into numerical vectors that computers can understand and compare efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting language to numbers\n",
    "\n",
    "\n",
    "Embeddings are numerical representations of text that capture semantic meaning. They convert words or sentences into vectors (lists of numbers) that can be compared mathematically.\n",
    "\n",
    "- Similar texts should have similar vector representations\n",
    "- The dimensionality and quality of embeddings affects performance\n",
    "- Different embedding models are trained on different data\n",
    "\n",
    "\n",
    "Embeddings transform text into numbers through several steps:\n",
    "\n",
    "1. **Tokenization**: Breaking text into smaller pieces\n",
    "   - Words: \"hello world\" → [\"hello\", \"world\"]\n",
    "   - Subwords: \"playing\" → [\"play\", \"##ing\"]\n",
    "   - Characters: For languages like Chinese\n",
    "\n",
    "2. **Token IDs**: Each token gets mapped to a number\n",
    "   - Example: \"hello\" → 15234\n",
    "   - These mappings are stored in the model's vocabulary\n",
    "\n",
    "3. **Neural Network Processing**:\n",
    "   - Tokens pass through multiple transformer layers\n",
    "   - Each layer learns different aspects (syntax, semantics, context)\n",
    "   - Final layer outputs the embedding vector\n",
    "\n",
    "4. **Vector Space**: The final embedding places similar meanings close together\n",
    "   - Each dimension captures different semantic features\n",
    "   - Typical sizes: 384, 768, or 1024 dimensions\n",
    "\n",
    "\n",
    "For this workshop we will be using sentences-tranformers, a popular open-soruce library for working with text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "text = \"I love machine learning\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# 2. Getting Embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding = model.encode(text)\n",
    "\n",
    "print(\"\\nEmbedding shape:\", embedding.shape)\n",
    "print(\"First 5 dimensions:\", embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Embedding Dimensions\n",
    "\n",
    "Each dimension in an embedding vector captures different semantic features:\n",
    "- Word types (noun, verb, etc.)\n",
    "- Topics (technology, nature, etc.)\n",
    "- Sentiment (positive, negative)\n",
    "- And many other abstract features\n",
    "\n",
    "The more dimensions, the more nuanced the representation, but also:\n",
    "- More computational cost\n",
    "- More storage needed\n",
    "- Risk of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_it(func, runs=3):\n",
    "    \"\"\"Time a function over multiple runs\"\"\"\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        func()\n",
    "        times.append(time.time() - start)\n",
    "    return f\"{(sum(times) / runs) * 1000:.2f}ms\"\n",
    "\n",
    "small_model = SentenceTransformer('all-MiniLM-L6-v2')  # 384 dims\n",
    "large_model = SentenceTransformer('all-mpnet-base-v2')  # 768 dims\n",
    "\n",
    "text = \"This is a test sentence\"\n",
    "small_emb = small_model.encode(text)\n",
    "large_emb = large_model.encode(text)\n",
    "\n",
    "print(f\"Small model dimensions: {small_emb.shape}\")\n",
    "print(f\"Large model dimensions: {large_emb.shape}\")\n",
    "print(f\"Small model speed: {time_it(lambda: small_model.encode(text))}\")\n",
    "print(f\"Large model speed: {time_it(lambda: large_model.encode(text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how embeddings group similar concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create word groups with clear relationships\n",
    "words = {\n",
    "    'food': ['pizza', 'burger', 'pasta', 'sushi'],\n",
    "    'drinks': ['coffee', 'tea', 'juice', 'water'],\n",
    "    'colors': ['red', 'blue', 'green', 'yellow']\n",
    "}\n",
    "\n",
    "# Get embeddings for all words\n",
    "all_words = [word for group in words.values() for word in group]\n",
    "embeddings = model.encode(all_words)\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot with different colors for each group\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#FF9999', '#66B2FF', '#99FF99']\n",
    "for (group_name, group_words), color in zip(words.items(), colors):\n",
    "    start_idx = all_words.index(group_words[0])\n",
    "    x = embeddings_2d[start_idx:start_idx+4, 0]\n",
    "    y = embeddings_2d[start_idx:start_idx+4, 1]\n",
    "    plt.scatter(x, y, c=color, label=group_name)\n",
    "    \n",
    "    # Add word labels\n",
    "    for i, word in enumerate(group_words):\n",
    "        plt.annotate(word, (x[i], y[i]))\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "plt.title(\"Word Embeddings Visualized in 2D\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Show some similarity scores\n",
    "print(\"\\nSimilarity Scores (closer to 1 = more similar):\")\n",
    "print(f\"pizza-burger: {cosine_similarity(model.encode('pizza'), model.encode('burger')):.3f}\")\n",
    "print(f\"coffee-tea: {cosine_similarity(model.encode('coffee'), model.encode('tea')):.3f}\")\n",
    "print(f\"pizza-coffee: {cosine_similarity(model.encode('pizza'), model.encode('coffee')):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why Different Models for Different Languages?\n",
    "\n",
    "Language-specific models perform better because:\n",
    "\n",
    "1. **Vocabulary Coverage**: \n",
    "   - Better handling of language-specific words\n",
    "   - Proper subword tokenization for morphologically rich languages\n",
    "\n",
    "2. **Cultural Context**:\n",
    "   - Understanding idioms and expressions\n",
    "   - Proper handling of formal/informal speech\n",
    "\n",
    "3. **Syntactic Structure**:\n",
    "   - Word order differences\n",
    "   - Grammar patterns\n",
    "\n",
    "This is why we use different models in our code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Tips for Using Embeddings\n",
    "\n",
    "1. **Choosing Model Size**:\n",
    "   - Smaller models (384 dims): Faster, good for simple tasks\n",
    "   - Larger models (768+ dims): Better quality, slower\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - Clean text (remove noise)\n",
    "   - Consistent casing\n",
    "   - Handle special characters\n",
    "\n",
    "3. **Storage Considerations**:\n",
    "   - 384 dimensions × 4 bytes = ~1.5KB per embedding\n",
    "   - Plan database capacity accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load different embedding models\n",
    "english_model = SentenceTransformer('all-MiniLM-L6-v2')  # English-focused\n",
    "german_model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True) # Multi-Lingual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Comparing Similar Sentences\n",
    "\n",
    "Let's see how embeddings capture similarity between sentences in different languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def compare_sentences(model, sent1, sent2):\n",
    "    emb1 = model.encode(sent1)\n",
    "    emb2 = model.encode(sent2)\n",
    "    return cosine_similarity(emb1, emb2)\n",
    "\n",
    "# Test pairs in different languages\n",
    "english_pairs = [\n",
    "    (\"I love Berlin\", \"Berlin is my favorite city\"),\n",
    "    (\"I love Berlin\", \"I hate vegetables\")\n",
    "]\n",
    "\n",
    "german_pairs = [\n",
    "    (\"Ich liebe Berlin\", \"Berlin ist meine Lieblingsstadt\"),\n",
    "    (\"Ich liebe Berlin\", \"Ich hasse Gemüse\")\n",
    "]\n",
    "\n",
    "print(\"\\nEnglish Model Results:\")\n",
    "for sent1, sent2 in english_pairs:\n",
    "    sim = compare_sentences(english_model, sent1, sent2)\n",
    "    print(f\"{sent1} <-> {sent2}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nGerman-English Model Results:\")\n",
    "for sent1, sent2 in german_pairs:\n",
    "    sim = compare_sentences(german_model, sent1, sent2)\n",
    "    print(f\"{sent1} <-> {sent2}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Cross-lingual Capabilities\n",
    "\n",
    "Let's compare how different models handle cross-lingual similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_lingual_pairs = [\n",
    "    (\"The weather is nice today\", \"Das Wetter ist heute schön\"),\n",
    "    (\"I need a coffee\", \"Ich brauche einen Kaffee\"),\n",
    "    (\"Berlin is the capital of Germany\", \"Berlin ist die Hauptstadt von Deutschland\")\n",
    "]\n",
    "\n",
    "print(\"Cross-lingual similarity:\")\n",
    "print(\"\\nEnglish-only model:\")\n",
    "for en, de in cross_lingual_pairs:\n",
    "    sim = compare_sentences(english_model, en, de)\n",
    "    print(f\"{en} <-> {de}: {sim:.3f}\")\n",
    "\n",
    "print(\"\\nGerman-English model:\")\n",
    "for en, de in cross_lingual_pairs:\n",
    "    sim = compare_sentences(german_model, en, de)\n",
    "    print(f\"{en} <-> {de}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Domain-Specific Comparisons\n",
    "\n",
    "Let's see how models handle domain-specific terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_pairs = [\n",
    "    (\"Guided tour of the Brandenburg Gate\", \"Führung durch das Brandenburger Tor\"),\n",
    "    (\"Skip the line tickets for museums\", \"Eintrittskarten ohne Anstehen für Museen\"),\n",
    "    (\"Best restaurants in Berlin\", \"Beste Restaurants in Berlin\")\n",
    "]\n",
    "\n",
    "print(\"Tourism domain comparisons:\")\n",
    "for en, de in tourism_pairs:\n",
    "    en_sim = compare_sentences(english_model, en, de)\n",
    "    de_sim = compare_sentences(german_model, en, de)\n",
    "    print(f\"\\nPair: {en} <-> {de}\")\n",
    "    print(f\"English model similarity: {en_sim:.3f}\")\n",
    "    print(f\"German-English model similarity: {de_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Language Support**: Models trained on specific languages perform better for those languages\n",
    "2. **Cross-lingual Capabilities**: Specialized multilingual models handle cross-language comparisons better\n",
    "3. **Domain Relevance**: Consider your use case when choosing an embedding model\n",
    "\n",
    "When building a RAG system:\n",
    "- Choose embedding models that match your content languages\n",
    "- Consider using specialized models for specific domains\n",
    "- Test different models with your actual use cases (And evaluate those)\n",
    "- Balance performance vs computational cost\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn to play with Embedding Models!\n",
    "\n",
    "## Exercise: Building a Smart Support System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_most_similar(query, message_list, model, top_k=1):\n",
    "    \"\"\"\n",
    "    Find the most similar customer messages\n",
    "    Returns: List of (message, score) tuples\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(query)\n",
    "    message_embeddings = model.encode(message_list)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = [\n",
    "        cosine_similarity(query_embedding, msg_emb) \n",
    "        for msg_emb in message_embeddings\n",
    "    ]\n",
    "    \n",
    "    # Get top-k matches\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [(message_list[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "support_messages = {\n",
    "    'account': [\n",
    "        \"How do I reset my password?\",\n",
    "        \"I can't log into my account\",\n",
    "        \"How do I change my email?\",\n",
    "        \"My account is locked\"\n",
    "    ],\n",
    "    'shipping': [\n",
    "        \"Where's my order?\",\n",
    "        \"Do you ship internationally?\",\n",
    "        \"How do I track my package?\",\n",
    "        \"Can I change my shipping address?\"\n",
    "    ],\n",
    "    'billing': [\n",
    "        \"How do I update my payment method?\",\n",
    "        \"Can I get a refund?\",\n",
    "        \"When will I be charged?\",\n",
    "        \"How do I cancel my subscription?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "test_queries = [\n",
    "    \"forgot password\",\n",
    "    \"package not here yet\",\n",
    "    \"want my money back\"\n",
    "]\n",
    "\n",
    "print(\"Basic similarity search:\")\n",
    "for query in test_queries:\n",
    "    matches = find_most_similar(query, [msg for msgs in support_messages.values() for msg in msgs], model, top_k=2)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    for match, score in matches:\n",
    "        print(f\"Match: '{match}' (Score: {score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try these experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Category Classification\n",
    "def classify_message(query, categories=support_messages):\n",
    "    \"\"\"\n",
    "    Your turn! Implement a function that:\n",
    "    1. Takes a customer query\n",
    "    2. Returns the most likely category (account/shipping/billing)\n",
    "    Hint: Compare query with all messages in each category\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# 2. Threshold Testing\n",
    "def is_relevant_query(query, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Your turn! Implement a function that:\n",
    "    1. Checks if a query is relevant to our support topics\n",
    "    2. Returns True if similarity is above threshold\n",
    "    Try with queries like \"What's the weather?\" vs \"Reset password\"\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# 4. Bonus: Multi-Language Support\n",
    "# Try adding some non-English support messages and queries!\n",
    "# Does the model handle them well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chunking\n",
    "\n",
    "## Why Do We Need Chunking?\n",
    "\n",
    "In the previous notebook, we saw how RAG systems use external information to make LLMs smarter. But there's a catch - we might not want to just feed entire documents into our system. We may want to break them into smaller, manageable pieces (chunks) because:\n",
    "\n",
    "1. **Context Window Limits**: LLMs have a maximum amount of text they can process at once\n",
    "2. **Relevant Information**: Smaller chunks help us find and retrieve only the most relevant parts of a document\n",
    "3. **Better Matching**: It's easier to match a question with a small, focused chunk than an entire document\n",
    "\n",
    "For example, if someone asks \"What is RAG?\" we want to find and use just the RAG-related chunk, not the entire document about AI concepts.\n",
    "\n",
    "## Chunking Strategies\n",
    "\n",
    "We'll explore two main ways to break up text:\n",
    "1. Simple character-based chunking (split every X characters)\n",
    "2. Smarter paragraph-based chunking (respect natural text boundaries)\n",
    "\n",
    "Let's see how these work in practice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load our data\n",
    "with open('../data/data_example.md', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Character-based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_characters(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into chunks of roughly equal size\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Get chunk with specified size\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # If we're not at the end, try to break at a period or newline\n",
    "        if end < len(text):\n",
    "            last_period = chunk.rfind('.')\n",
    "            last_newline = chunk.rfind('\\n')\n",
    "            break_point = max(last_period, last_newline)\n",
    "            \n",
    "            if break_point != -1:\n",
    "                chunk = chunk[:break_point + 1]\n",
    "                end = start + break_point + 1\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Let's try it out\n",
    "char_chunks = chunk_by_characters(text, chunk_size=500)\n",
    "print(f\"Number of character-based chunks: {len(char_chunks)}\")\n",
    "print(\"\\nFirst two chunks:\")\n",
    "print(\"Chunk 1:\", char_chunks[0], \"\\n\")\n",
    "print(\"Chunk 2:\", char_chunks[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Smarter Paragraph-based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_by_paragraphs(text, max_chars=1000):\n",
    "    \"\"\"\n",
    "    Split text into chunks based on paragraphs\n",
    "    \"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) <= max_chars:\n",
    "            current_chunk += para + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = para + \"\\n\\n\"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Try paragraph-based chunking\n",
    "para_chunks = chunk_by_paragraphs(text)\n",
    "print(f\"Number of paragraph-based chunks: {len(para_chunks)}\")\n",
    "print(\"\\nFirst two chunks:\")\n",
    "print(\"Chunk 1:\", para_chunks[0], \"\\n\")\n",
    "print(\"Chunk 2:\", para_chunks[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn! Experiment with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try these experiments:\n",
    "# 1. Change chunk sizes (try 200, 1000 characters)\n",
    "# 2. Modify overlap (try 0, 100 characters)\n",
    "# 3. Compare the results\n",
    "\n",
    "# Example with different parameters:\n",
    "small_chunks = chunk_by_characters(text, chunk_size=200, overlap=0)\n",
    "large_chunks = chunk_by_characters(text, chunk_size=1000, overlap=100)\n",
    "\n",
    "print(\"Small chunks (200 chars, no overlap):\")\n",
    "print(f\"Number of chunks: {len(small_chunks)}\")\n",
    "print(\"First chunk:\", small_chunks[0], \"\\n\")\n",
    "\n",
    "print(\"Large chunks (1000 chars, 100 char overlap):\")\n",
    "print(f\"Number of chunks: {len(large_chunks)}\")\n",
    "print(\"First chunk:\", large_chunks[0], \"\\n\")\n",
    "\n",
    "# Discussion points:\n",
    "# - How does chunk size affect the coherence of the text?\n",
    "# - What happens to the meaning when chunks are too small?\n",
    "# - How does overlap help maintain context between chunks?\n",
    "# - Which method works better for this specific text? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pratical RAG example\n",
    "\n",
    "Now that we have learned about Embeddings and Chunking, let's run a Pratical RAG example combining the concepts we learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG implementation combining embeddings and chunks\n",
    "def simple_rag(query, chunks, model):\n",
    "    # 1. Embed query and chunks\n",
    "    query_emb = model.encode(query)\n",
    "    chunk_embs = model.encode(chunks)\n",
    "    \n",
    "    # 2. Find most relevant chunk\n",
    "    similarities = [cosine_similarity(query_emb, chunk_emb) for chunk_emb in chunk_embs]\n",
    "    best_chunk = chunks[np.argmax(similarities)]\n",
    "    \n",
    "    return best_chunk\n",
    "\n",
    "questions = [\n",
    "    \"What are the benefits of RAG?\",\n",
    "    \"How do embeddings work?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    relevant_chunk = simple_rag(q, para_chunks, model)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"Relevant chunk: {relevant_chunk[:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
